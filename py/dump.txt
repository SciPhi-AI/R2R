diff --git a/py/core/main/api/v3/documents_router.py b/py/core/main/api/v3/documents_router.py
index 1bf8e473..7d334f2f 100644
--- a/py/core/main/api/v3/documents_router.py
+++ b/py/core/main/api/v3/documents_router.py
@@ -1277,7 +1277,9 @@ class DocumentsRouter(BaseRouterV3):
         )
         @self.base_endpoint
         async def delete_document_by_filter(
-            filters: Json[dict] = Body(..., description="JSON-encoded filters"),
+            filters: Json[dict] = Body(
+                ..., description="JSON-encoded filters"
+            ),
             auth_user=Depends(self.providers.auth.auth_wrapper()),
         ) -> WrappedBooleanResponse:
             """
diff --git a/py/core/main/services/ingestion_service.py b/py/core/main/services/ingestion_service.py
index 154d9f12..148ca7d2 100644
--- a/py/core/main/services/ingestion_service.py
+++ b/py/core/main/services/ingestion_service.py
@@ -32,7 +32,7 @@ from core.base.abstractions import (
 )
 from core.base.api.models import User
 from core.telemetry.telemetry_decorator import telemetry_event
-from shared.abstractions import PDFParsingError, PopperNotFoundError
+from shared.abstractions import PDFParsingError, PopplerNotFoundError
 
 from ..abstractions import R2RProviders
 from ..config import R2RConfig
@@ -274,7 +274,7 @@ class IngestionService:
                 extraction.metadata["version"] = version
                 yield extraction
 
-        except (PopperNotFoundError, PDFParsingError) as e:
+        except (PopplerNotFoundError, PDFParsingError) as e:
             raise R2RDocumentProcessingError(
                 error_message=e.message,
                 document_id=document_info.id,
diff --git a/py/core/parsers/media/pdf_parser.py b/py/core/parsers/media/pdf_parser.py
index a09d025d..d2bd8df9 100644
--- a/py/core/parsers/media/pdf_parser.py
+++ b/py/core/parsers/media/pdf_parser.py
@@ -24,7 +24,7 @@ from core.base.providers import (
     DatabaseProvider,
     IngestionConfig,
 )
-from shared.abstractions import PDFParsingError, PopperNotFoundError
+from shared.abstractions import PDFParsingError, PopplerNotFoundError
 
 logger = logging.getLogger()
 
@@ -75,7 +75,7 @@ class VLMPDFParser(AsyncParser[str | bytes]):
             logger.error(
                 "PDFInfoNotInstalledError encountered during PDF conversion."
             )
-            raise PopperNotFoundError()
+            raise PopplerNotFoundError()
         except Exception as err:
             logger.error(
                 f"Error converting PDF to images: {err} type: {type(err)}"
@@ -130,7 +130,7 @@ class VLMPDFParser(AsyncParser[str | bytes]):
             if response.choices and response.choices[0].message:
                 content = response.choices[0].message.content
                 page_elapsed = time.perf_counter() - page_start
-                logger.info(
+                logger.debug(
                     f"Processed page {page_num} in {page_elapsed:.2f} seconds."
                 )
                 return {"page": str(page_num), "content": content}
@@ -146,7 +146,7 @@ class VLMPDFParser(AsyncParser[str | bytes]):
 
     async def ingest(
         self, data: str | bytes, maintain_order: bool = True, **kwargs
-    ) -> AsyncGenerator[str, None]:
+    ) -> AsyncGenerator[dict[str, str | int], None]:
         """
         Ingest PDF data and yield the text description for each page using the vision model.
         (This version yields a string per page rather than a dictionary.)
@@ -185,16 +185,21 @@ class VLMPDFParser(AsyncParser[str | bytes]):
                         result = await task
                         page_num = int(result["page"])
                         results[page_num] = result
-                        # **Fix:** Yield only the content string instead of the whole dictionary.
                         while next_page in results:
-                            yield results.pop(next_page)["content"]
+                            yield {
+                                "content": results[next_page]["content"],
+                                "page_number": next_page,
+                            }
+                            results.pop(next_page)
                             next_page += 1
             else:
                 # Yield results as tasks complete
                 for coro in asyncio.as_completed(tasks.keys()):
                     result = await coro
-                    # **Fix:** Yield only the content string.
-                    yield result["content"]
+                    yield {
+                        "content": result["content"],
+                        "page_number": int(result["page"]),
+                    }
             total_elapsed = time.perf_counter() - ingest_start
             logger.info(
                 f"Completed PDF ingestion in {total_elapsed:.2f} seconds using VLMPDFParser."
diff --git a/py/core/providers/ingestion/r2r/base.py b/py/core/providers/ingestion/r2r/base.py
index 0f985f08..3ca7dfe8 100644
--- a/py/core/providers/ingestion/r2r/base.py
+++ b/py/core/providers/ingestion/r2r/base.py
@@ -221,6 +221,8 @@ class R2RIngestionProvider(IngestionProvider):
         document: Document,
         ingestion_config_override: dict,
     ) -> AsyncGenerator[DocumentChunk, None]:
+        
+        print(f"Got ingestion_config_override: {ingestion_config_override}")
         if document.document_type not in self.parsers:
             raise R2RDocumentProcessingError(
                 document_id=document.id,
@@ -228,7 +230,7 @@ class R2RIngestionProvider(IngestionProvider):
             )
         else:
             t0 = time.time()
-            contents = ""
+            contents = []
             parser_overrides = ingestion_config_override.get(
                 "parser_overrides", {}
             )
@@ -244,37 +246,48 @@ class R2RIngestionProvider(IngestionProvider):
                     raise ValueError(
                         "Only Zerox PDF parser override is available."
                     )
-                async for text in self.parsers[
+                async for chunk in self.parsers[
                     f"zerox_{DocumentType.PDF.value}"
                 ].ingest(file_content, **ingestion_config_override):
-                    if text is not None:
-                        contents += text + "\n"
+                    if isinstance(chunk, dict) and chunk.get("content"):
+                        contents.append(chunk)
+                    elif (
+                        chunk
+                    ):  # Handle string output for backward compatibility
+                        contents.append({"content": chunk})
             else:
                 async for text in self.parsers[document.document_type].ingest(
                     file_content, **ingestion_config_override
                 ):
                     if text is not None:
-                        contents += text + "\n"
+                        contents.append({"content": text})
 
-            if not contents.strip():
+            if not contents:
                 logging.warning(
                     "No valid text content was extracted during parsing"
                 )
                 return
 
             iteration = 0
-            chunks = self.chunk(contents, ingestion_config_override)
-            for chunk in chunks:
-                extraction = DocumentChunk(
-                    id=generate_extraction_id(document.id, iteration),
-                    document_id=document.id,
-                    owner_id=document.owner_id,
-                    collection_ids=document.collection_ids,
-                    data=chunk,
-                    metadata={**document.metadata, "chunk_order": iteration},
-                )
-                iteration += 1
-                yield extraction
+            for content_item in contents:
+                chunk_text = content_item["content"]
+                chunks = self.chunk(chunk_text, ingestion_config_override)
+
+                for chunk in chunks:
+                    metadata = {**document.metadata, "chunk_order": iteration}
+                    if "page_number" in content_item:
+                        metadata["page_number"] = content_item["page_number"]
+
+                    extraction = DocumentChunk(
+                        id=generate_extraction_id(document.id, iteration),
+                        document_id=document.id,
+                        owner_id=document.owner_id,
+                        collection_ids=document.collection_ids,
+                        data=chunk,
+                        metadata=metadata,
+                    )
+                    iteration += 1
+                    yield extraction
 
             logger.debug(
                 f"Parsed document with id={document.id}, title={document.metadata.get('title', None)}, "
diff --git a/py/shared/abstractions/__init__.py b/py/shared/abstractions/__init__.py
index bcbdb2a5..3f6068a0 100644
--- a/py/shared/abstractions/__init__.py
+++ b/py/shared/abstractions/__init__.py
@@ -14,7 +14,7 @@ from .document import (
 from .embedding import EmbeddingPurpose, default_embedding_prefixes
 from .exception import (
     PDFParsingError,
-    PopperNotFoundError,
+    PopplerNotFoundError,
     R2RDocumentProcessingError,
     R2RException,
 )
@@ -92,7 +92,7 @@ __all__ = [
     "R2RDocumentProcessingError",
     "R2RException",
     "PDFParsingError",
-    "PopperNotFoundError",
+    "PopplerNotFoundError",
     # Graph abstractions
     "Entity",
     "Community",
diff --git a/py/shared/abstractions/exception.py b/py/shared/abstractions/exception.py
index d368856b..67eaffca 100644
--- a/py/shared/abstractions/exception.py
+++ b/py/shared/abstractions/exception.py
@@ -51,7 +51,7 @@ class PDFParsingError(R2RException):
         super().__init__(message, status_code, detail)
 
 
-class PopperNotFoundError(PDFParsingError):
+class PopplerNotFoundError(PDFParsingError):
     """Specific error for when Poppler is not installed."""
 
     def __init__(self):

---
title: 'RAG'
description: 'Learn how to configure RAG in your R2R deployment'
icon: 'brain'
---

## RAG Customization

RAG (Retrieval-Augmented Generation) in R2R can be extensively customized to suit various use cases. The main components for customization are:

1. **Generation Configuration**: Control the language model's behavior.
2. **Search Settings**: Fine-tune the retrieval process.
3. **Task Prompt Override**: Customize the system prompt for specific tasks.


### LLM Provider Configuration

Refer to the LLM configuration [page here](/documentation/configuration/llm).


### Search Configuration

Refer to the search configuration [page here](/documentation/configuration/retrieval/overview).


### Combining Search and Generation



The `rag_generation_config` parameter allows you to customize the language model's behavior. Default settings are set on the server-side using the `r2r.toml`, as described in the LLM configuration guide, and they can be overridden at runtime as shown below:

```python
rag_generation_config = {
    "model": "anthropic/claude-3-opus-20240229",
    "temperature": 0.7,
    "top_p": 0.95,
    "max_tokens_to_sample": 1500,
    "stream": True,
    "functions": None,  # For function calling, if supported
    "tools": None,  # For tool use, if supported
    "add_generation_kwargs": {},  # Additional provider-specific parameters
    "api_base": None  # Custom API endpoint, if needed
}
```

Similarly, the vector search and knowledge graph settings can be overridden at runtime.

When performing a RAG query you can combine vector search, knowledge graph search, and generation settings:

```python
from r2r import R2RClient

client = R2RClient()

response = client.rag(
    "What are the latest advancements in quantum computing?",
    rag_generation_config=rag_generation_config
    vector_search_settings=vector_search_settings,
    kg_search_settings=kg_search_settings,
)
```

### Task Prompt Override

For specialized tasks, you can override the default system prompt:

```python
task_prompt_override = """You are an AI assistant specializing in quantum computing.
Your task is to provide a concise summary of the latest advancements in the field,
focusing on practical applications and breakthroughs from the past year."""

response = client.rag(
    "What are the latest advancements in quantum computing?",
    rag_generation_config=rag_generation_config,
    task_prompt_override=task_prompt_override
)
```

## Agent-based Interaction

R2R supports multi-turn conversations and complex query processing through its agent endpoint:

```python
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "What are the key differences between quantum and classical computing?"}
]

response = client.agent(
    messages=messages,
    vector_search_settings=vector_search_settings,
    kg_search_settings=kg_search_settings,
    rag_generation_config=rag_generation_config,
)
```

The agent can break down complex queries into sub-tasks, leveraging both retrieval and generation capabilities to provide comprehensive responses.

By leveraging these configuration options, you can fine-tune R2R's retrieval and generation process to best suit your specific use case and requirements.



## Next Steps

For more detailed information on configuring specific components of R2R, please refer to the following pages:

- [Postgres Configuration](/documentation/configuration/postgres)
- [LLM Configuration](/documentation/configuration/llm)
- [Ingestion Configuration](/documentation/configuration/ingestion/overview)
- [Knowledge Graph Configuration](/documentation/configuration/knowledge-graph/overview)
- [Retrieval Configuration](/documentation/configuration/retrieval/overview)

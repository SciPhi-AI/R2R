---
title: 'Ingestion'
description: 'Configure the R2R ingestion pipeline'
---
## Introduction

R2R's ingestion pipeline processes and stores documents efficiently for later retrieval. This guide covers the key configuration options for the ingestion process, focusing on embedding provider configuration.

We have the following components available for configuration:

- [Parsing Provider](#parsing-provider)
- [Chunking Provider](#chunking-provider)
- [Embedding Provider](#embedding-provider)
- [Knowledge Graph Provider](#knowledge-graph-provider)
- [Vector & Relational Databases](#vector--relational-databases)


## Parsing Provider

R2R supports different parsing providers to extract text from various document formats. To configure the parsing provider:

```toml example r2r.toml
[parsing]
provider = "r2r"
excluded_parsers = ["mp4"]
```
Available providers:
- `r2r`: Default parser provided by R2R
- `unstructured`: Local installation of the Unstructured library (requires additional setup)
- `unstructured_api`: Cloud offering of Unstructured

### Supported File Types

**R2R supports parsing for the following file types:**

- CSV (Comma-Separated Values)
- DOCX (Microsoft Word Document)
- HTML (HyperText Markup Language)
- JSON (JavaScript Object Notation)
- MD (Markdown)
- PDF (Portable Document Format)
- PPTX (Microsoft PowerPoint Presentation)
- TXT (Plain Text)
- XLSX (Microsoft Excel Spreadsheet)
- GIF (Graphics Interchange Format)
- JPEG/JPG (Joint Photographic Experts Group)
- PNG (Portable Network Graphics)
- SVG (Scalable Vector Graphics)
- MP3 (MPEG Audio Layer III)
- MP4 (MPEG-4 Part 14)

<Note> Parsing providers for an R2R system cannot be configured at runtime and are instead configured server side. </Note>

**Refer to the [Unstructured documentation](https://docs.unstructured.io/welcome) for details about their ingestion capabilities and limitations.**


## Chunking Provider

Chunking is the process of breaking down parsed documents into smaller, manageable pieces. Configure the chunking provider as follows:


```toml r2r.toml
[chunking]
provider = "r2r"
method = "recursive"
chunk_size = 512
chunk_overlap = 50
```

### Supported Chunkers
- `r2r`: Default chunking method provided by R2R
- `unstructured`: Chunking methods from the Unstructured library (requires additional installation)

Chunking methods:
- `by_title`, supported by unstructured
- `basic`, supported by unstructured
- `recursive`, supported by R2R

Chunking providers can be specified at runtime with the [`ingest_files`](api-reference/endpoint/ingest_files) endpoint.


## Embedding Provider

By default, R2R uses the LiteLLM framework to communicate with various cloud embedding providers. To customize the embedding settings:

1. Edit the `embedding` section in your `r2r.toml` file:

```toml r2r.toml
[embedding]
provider = "litellm"
base_model = "openai/text-embedding-3-small"
base_dimension = 512
batch_size = 128
add_title_as_prefix = false
rerank_model = "None"
concurrent_request_limit = 256
```

2. Launch R2R with your configuration:
```
r2r serve --config-path=r2r.toml
```

Let's break down the embedding configuration options:

- `provider`: Choose from `ollama`, `litellm` and `openai`. R2R defaults to using the LiteLLM framework for maximum embedding provider flexibility.
- `base_model`: Specifies the embedding model to use. Format is typically "provider/model-name" (e.g., `"openai/text-embedding-3-small"`).
- `base_dimension`: Sets the dimension of the embedding vectors. Should match the output dimension of the chosen model.
- `batch_size`: Determines the number of texts to embed in a single API call. Larger values can improve throughput but may increase latency.
- `add_title_as_prefix`: When true, prepends the document title to the text before embedding, providing additional context.
- `rerank_model`: Specifies a model for reranking results. Set to "None" to disable reranking (note: not supported by LiteLLMEmbeddingProvider).
- `concurrent_request_limit`: Sets the maximum number of concurrent embedding requests to manage load and avoid rate limiting.

<Note> Embedding providers for an R2R system cannot be configured at runtime and are instead configured server side. </Note>


### Supported Providers

<Tabs>
  <Tab title="OpenAI">
    Example configuration:
    ```toml example r2r.toml
    provider = "litellm"
    base_model = "text-embedding-3-small"
    base_dimension = 512
    ```

    ```bash
    export OPENAI_API_KEY=your_openai_key
    # .. set other environment variables

    r2r serve --config-path=r2r.toml
    ```
    Supported models include:
    - text-embedding-3-small
    - text-embedding-3-large
    - text-embedding-ada-002

    For detailed usage instructions, refer to the [LiteLLM OpenAI Embedding documentation](https://docs.litellm.ai/docs/embedding/supported_embedding#openai-embedding-models).
  </Tab>

  <Tab title="Azure">
    Example configuration:
    ```toml example r2r.toml
    provider = "litellm"
    base_model = "azure/<your deployment name>"
    base_dimension = XXX
    ```

    ```bash
    export AZURE_API_KEY=your_azure_api_key
    export AZURE_API_BASE=your_azure_api_base
    export AZURE_API_VERSION=your_azure_api_version
    # .. set other environment variables

    r2r serve --config-path=r2r.toml
    ```
    Supported models include:
    - text-embedding-ada-002

    For detailed usage instructions, refer to the [LiteLLM Azure Embedding documentation](https://docs.litellm.ai/docs/embedding/supported_embedding#azure-openai-embedding-models).
  </Tab>

  <Tab title="Anthropic">
    Anthropic does not currently offer embedding models. Consider using OpenAI or another provider for embeddings.
  </Tab>

  <Tab title="Cohere">
    Example configuration:
    ```toml example r2r.toml
    provider = "litellm"
    base_model = "embed-english-v3.0"
    base_dimension = 1_024
    ```

    ```bash
    export COHERE_API_KEY=your_cohere_api_key
    # .. set other environment variables

    r2r serve --config-path=r2r.toml
    ```

    Supported models include:
    - embed-english-v3.0
    - embed-english-light-v3.0
    - embed-multilingual-v3.0
    - embed-multilingual-light-v3.0
    - embed-english-v2.0
    - embed-english-light-v2.0
    - embed-multilingual-v2.0

    For detailed usage instructions, refer to the [LiteLLM Cohere Embedding documentation](https://docs.litellm.ai/docs/embedding/supported_embedding#cohere-embedding-models).
  </Tab>

  <Tab title="Ollama">


    When running with Ollama, additional changes are recommended for the to the `r2r.toml` file. In addition to using the `ollama` provider directly, we recommend restricting the `concurrent_request_limit` in order to avoid exceeding the throughput of your Ollama server.
    ```toml example r2r.toml
    [embedding]
    provider = "ollama"
    base_model = "mxbai-embed-large"
    base_dimension = 1_024
    batch_size = 32
    add_title_as_prefix = true
    ```


    ```bash
    # Ensure your Ollama server is running
    # Default Ollama server address: http://localhost:11434
    # <-- OR -->
    # Use `r2r serve --docker --config-name=local_llm`
    # which bundles ollama with R2R in Docker by default!

    r2r serve --config-path=r2r.toml
    ```

    Then, deploy your R2R server with `r2r serve --config-path=r2r.toml`.
  </Tab>

  <Tab title="HuggingFace">
    Example configuration:

    ```toml example r2r.toml
    [embedding]
    provider = "litellm"
    base_model = "huggingface/microsoft/codebert-base"
    base_dimension = 768
    ```

    ```python
    export HUGGINGFACE_API_KEY=your_huggingface_api_key

    r2r serve --config-path=r2r.toml
    ```
    LiteLLM supports all Feature-Extraction Embedding models on HuggingFace.

    For detailed usage instructions, refer to the [LiteLLM HuggingFace Embedding documentation](https://docs.litellm.ai/docs/embedding/supported_embedding#huggingface-embedding-models).
  </Tab>

  <Tab title="Bedrock">
    Example configuration:

    ```toml example r2r.toml
    provider = "litellm"
    base_model = "amazon.titan-embed-text-v1"
    base_dimension = 1_024
    ```

    ```bash
    export AWS_ACCESS_KEY_ID=your_access_key
    export AWS_SECRET_ACCESS_KEY=your_secret_key
    export AWS_REGION_NAME=your_region_name
    # .. set other environment variables

    r2r serve --config-path=r2r.toml
    ```
    Supported models include:
    - amazon.titan-embed-text-v1
    - cohere.embed-english-v3
    - cohere.embed-multilingual-v3

    For detailed usage instructions, refer to the [LiteLLM Bedrock Embedding documentation](https://docs.litellm.ai/docs/embedding/supported_embedding#bedrock-embedding).
  </Tab>

  <Tab title="Vertex AI">

    Example configuration:
    ```toml example r2r.toml
    provider = "litellm"
    base_model = "vertex_ai/textembedding-gecko"
    base_dimension = 768
    ```

    ```bash
    export GOOGLE_APPLICATION_CREDENTIALS=path/to/your/credentials.json
    export VERTEX_PROJECT=your_project_id
    export VERTEX_LOCATION=your_project_location
    # .. set other environment variables

    r2r serve --config-path=r2r.toml
    ```
    Supported models include:
    - textembedding-gecko
    - textembedding-gecko-multilingual
    - textembedding-gecko@001
    - textembedding-gecko@003
    - text-embedding-preview-0409
    - text-multilingual-embedding-preview-0409

    For detailed usage instructions, refer to the [LiteLLM Vertex AI Embedding documentation](https://docs.litellm.ai/docs/embedding/supported_embedding#vertex-ai-embedding-models).
  </Tab>

  <Tab title="Voyage AI">
  Example Configuration
    ```toml example r2r.toml
    provider = "litellm"
    base_model = "voyage/voyage-01"
    base_dimension = 1_024
    ```

    ```bash
    export VOYAGE_API_KEY=your_voyage_api_key
    # .. set other environment variables

    r2r serve --config-path=r2r.toml
    ```
    Supported models include:
    - voyage-01
    - voyage-lite-01
    - voyage-lite-01-instruct

    For detailed usage instructions, refer to the [LiteLLM Voyage AI Embedding documentation](https://docs.litellm.ai/docs/embedding/supported_embedding#voyage-ai-embedding-models).
  </Tab>
</Tabs>


## Knowledge Graph Provider

R2R supports knowledge graph functionality to enhance document understanding and retrieval. By default, R2R uses Neo4j as the knowledge graph provider. To configure the knowledge graph settings:

1. Edit the `kg` section in your `r2r.toml` file:

```toml r2r.toml
[kg]
provider = "neo4j"
batch_size = 256
kg_extraction_prompt = "graphrag_triplet_extraction_zero_shot"

  [kg.kg_extraction_config]
    model = "gpt-4o-mini" # and other generation params

  [kg.kg_enrichment_config]
    max_knowledge_triples = 100 # max number of triples to extract for each document chunk
    generation_config = { model = "gpt-4o-mini" } # and other generation params
    leiden_params = { max_cluster_size = 1000 } # more params in graspologic/partition/leiden.py

  [kg.kg_search_config]
    model = "gpt-4o-mini"
```


Let's break down the knowledge graph configuration options:

- `provider`: Specifies the knowledge graph provider. Currently, "neo4j" is supported.
- `batch_size`: Determines the number of entities or relationships to process in a single batch during import operations.
- `kg_extraction_prompt`: Specifies the prompt template to use for extracting knowledge graph information from text.
- `kg_extraction_config`: Configuration for the model used in knowledge graph extraction.
  - `model`: The model to use for knowledge graph extraction (e.g., "gpt-4o-mini").
  - `temperature`: Controls the randomness of the model's output.
  - `top_p`: Nucleus sampling parameter.
  - `max_tokens_to_sample`: Maximum number of tokens to generate in the response.
  - `stream`: Whether to stream the model's output.
  - `add_generation_kwargs`: Additional keyword arguments for the generation process.
- `kg_search_config`: Similar configuration for the model used in knowledge graph search operations.

### Neo4j Configuration

When using Neo4j as the knowledge graph provider, you need to set up the following environment variables or provide them in the `r2r.toml` file:

```bash
export NEO4J_USER=your_neo4j_username
export NEO4J_PASSWORD=your_neo4j_password
export NEO4J_URL=bolt://your_neo4j_host:7687
export NEO4J_DATABASE=neo4j # Optional, defaults to "neo4j"
```

```toml r2r.toml
[kg.extra_fields]
user = "your_neo4j_username"
password = "your_neo4j_password"
url = "bolt://your_neo4j_host:7687"
database = "neo4j" # Optional, defaults to "neo4j"
```


### Knowledge Graph Operations

The Neo4jKGProvider supports various operations:

1. **Entity Management**: Add, update, and retrieve entities in the knowledge graph.
2. **Relationship Management**: Create and query relationships between entities.
3. **Batch Import**: Efficiently import large amounts of data using batched operations.
4. **Vector Search**: Perform similarity searches on entity embeddings.
5. **Community Detection**: Identify and manage communities within the graph.

### Customization

You can customize the knowledge graph extraction and search processes by modifying the `kg_extraction_prompt` and adjusting the model configurations in `kg_extraction_config` and `kg_search_config`. Moreover, you can customize the LLM models used in various parts of the knowledge graph creation process. All of these options can be selected at runtime, with the only exception being the specified database provider. For more details, refer to the knowledge graph settings in the [search API](/api-reference/endpoint/search).

By leveraging the knowledge graph capabilities, you can enhance R2R's understanding of document relationships and improve the quality of search and retrieval operations.


## Vector & Relational Databases

# Postgres Configuration

R2R uses Postgres as its default database provider. To customize the database settings, you can modify the `database` section in your `r2r.toml` file and set corresponding environment variables or provide the settings directly in the configuration file.

1. Edit the `database` section in your `r2r.toml` file:

```toml r2r.toml
[database]
provider = "postgres"
  [database.extra_fields]
  user = "your_postgres_user"
  password = "your_postgres_password"
  host = "your_postgres_host"
  port = "your_postgres_port"
  db_name = "your_database_name"
  vecs_collection = "your_vector_collection_name"
```

2. Alternatively, you can set the following environment variables:

```bash
export POSTGRES_USER=your_postgres_user
export POSTGRES_PASSWORD=your_postgres_password
export POSTGRES_HOST=your_postgres_host
export POSTGRES_PORT=your_postgres_port
export POSTGRES_DBNAME=your_database_name
export POSTGRES_VECS_COLLECTION=your_vector_collection_name
```



Let's break down the Postgres configuration options:

- `user`: The username for connecting to your Postgres database.
- `password`: The password for the specified user.
- `host`: The hostname or IP address of your Postgres server.
- `port`: The port number on which Postgres is running (default is usually 5432).
- `db_name`: The name of the database you want to use for R2R.
- `vecs_collection`: The name of the collection to use for storing vector embeddings.

### Important Notes:

1. If you provide both environment variables and `extra_fields` in the `r2r.toml` file, the `extra_fields` values will take precedence.

2. Ensure that all required fields are set either through environment variables or in the `r2r.toml` file. If any required field is missing, R2R will raise a `ValueError` with a descriptive error message.

3. The database connection string is constructed as follows:
   ```
   postgresql://{user}:{password}@{host}:{port}/{db_name}
   ```

4. Make sure your Postgres database is properly set up and accessible with the provided credentials before starting R2R.

5. If you're using a cloud-hosted Postgres service, make sure to configure any necessary firewall rules or security groups to allow connections from your R2R application.

By customizing these settings, you can configure R2R to use your specific Postgres database instance, allowing for flexibility in your deployment setup and data management.

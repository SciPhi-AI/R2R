# How to Check R2R Logs and Use Analytics & Observability Features

This guide covers various methods to access and analyze R2R logs, as well as leverage R2R's powerful analytics and observability features. These capabilities allow you to monitor system performance, track usage patterns, and gain valuable insights into your RAG application's behavior.

## 1. Checking R2R Logs

### 1.1 Docker Deployment

If you're running R2R using Docker:

1. List running containers:
   ```bash
   docker ps
   ```

2. View real-time logs:
   ```bash
   docker logs -f <container_id_or_name>
   ```

3. Using Docker Compose:
   ```bash
   docker-compose logs -f r2r
   ```

### 1.2 Local Deployment

For local deployments without Docker:

1. Check the R2R configuration for log file locations.
2. Use standard Unix tools to view logs:
   ```bash
   tail -f /path/to/r2r.log
   ```

### 1.3 Cloud Deployments

- For Azure: Use "Log stream" or "Diagnose and solve problems" in the Azure portal.
- For AWS: Use CloudWatch or check EC2/ECS logs directly.

## 2. Using R2R's Logging and Analytics Features

<Note>
The features described in this section are typically restricted to superusers. Ensure you have the necessary permissions before attempting to access these features.
</Note>

### 2.1 Fetching Logs

You can fetch logs using the client-server architecture:

<Tabs>
<Tab title="CLI">
```bash
r2r logs
```
</Tab>

<Tab title="Python">
```python
client.logs()
```
</Tab>

<Tab title="JavaScript">
```javascript
client.logs()
```
</Tab>

<Tab title="Curl">
```bash
curl -X POST http://localhost:7272/v2/logs \
  -H "Content-Type: application/json" \
  -d '{
    "log_type_filter": null,
    "max_runs_requested": 100
  }'
```
</Tab>
</Tabs>

Expected Output:
```python
[
    {
        'run_id': UUID('27f124ad-6f70-4641-89ab-f346dc9d1c2f'),
        'run_type': 'rag',
        'entries': [
            {'key': 'search_query', 'value': 'Who is Aristotle?'},
            {'key': 'search_latency', 'value': '0.39'},
            {'key': 'search_results', 'value': '["{\\"id\\":\\"7ed3a01c-88dc-5a58-a68b-6e5d9f292df2\\",...}"]'},
            {'key': 'rag_generation_latency', 'value': '3.79'},
            {'key': 'llm_response', 'value': 'Aristotle (Greek: Ἀριστοτέλης Aristotélēs; 384–322 BC) was...'}
        ]
    },
    # More log entries...
]
```

### 2.2 Using Analytics

R2R offers an analytics feature for aggregating and analyzing log data:

<Tabs>
<Tab title="CLI">
```bash
r2r analytics --filters '{"search_latencies": "search_latency"}' --analysis-types '{"search_latencies": ["basic_statistics", "search_latency"]}'
```
</Tab>

<Tab title="Python">
```python
client.analytics(
  {"search_latencies": "search_latency"},
  {"search_latencies": ["basic_statistics", "search_latency"]}
)
```
</Tab>

<Tab title="JavaScript">
```javascript
const filterCriteria = {
    filters: {
      search_latencies: "search_latency",
    },
  };

const analysisTypes = {
  search_latencies: ["basic_statistics", "search_latency"],
};

client.analytics(filterCriteria, analysisTypes);
```
</Tab>

<Tab title="Curl">
```bash
curl -X POST http://localhost:7272/v2/analytics \
  -H "Content-Type: application/json" \
  -d '{
    "filter_criteria": {
      "filters": {
        "search_latencies": "search_latency"
      }
    },
    "analysis_types":
    {
        "analysis_types": {
            "search_latencies": ["basic_statistics", "search_latency"]
        }
    }
  }'
```
</Tab>
</Tabs>

Expected Output:
```python
{
    'results': {
        'filtered_logs': {
            'search_latencies': [
                {
                    'timestamp': '2024-06-20 21:29:06',
                    'log_id': UUID('0f28063c-8b87-4934-90dc-4cd84dda5f5c'),
                    'key': 'search_latency',
                    'value': '0.66',
                    'rn': 3
                },
                ...
            ]
        },
        'search_latencies': {
            'Mean': 0.734,
            'Median': 0.523,
            'Mode': 0.495,
            'Standard Deviation': 0.213,
            'Variance': 0.0453
        }
    }
}
```

## 3. Advanced Analytics and Observability

### 3.1 Custom Analytics

You can specify different filters and analysis types to focus on specific aspects of your application's performance:

```python
from r2r import FilterCriteria, AnalysisTypes

# Analyze RAG latencies
rag_filter = FilterCriteria(filters={"rag_latencies": "rag_generation_latency", "rag_eval": "rag_eval_metric"})
rag_analysis = AnalysisTypes(analysis_types={"rag_latencies": ["basic_statistics", "rag_generation_latency"]})
rag_analytics = app.analytics(rag_filter, rag_analysis)

# Track usage patterns by user
user_filter = FilterCriteria(filters={"user_patterns": "user_id"})
user_analysis = AnalysisTypes(analysis_types={"user_patterns": ["bar_chart", "user_id"]})
user_analytics = app.analytics(user_filter, user_analysis)

# Monitor error rates
error_filter = FilterCriteria(filters={"error_rates": "error"})
error_analysis = AnalysisTypes(analysis_types={"error_rates": ["basic_statistics", "error"]})
error_analytics = app.analytics(error_filter, error_analysis)
```

### 3.2 Preloading Data for Analysis

To get meaningful analytics, you can preload your database with random searches:

```python
import random
from r2r import R2R, GenerationConfig

app = R2R()

queries = [
    "What is artificial intelligence?",
    "Explain machine learning.",
    "How does natural language processing work?",
    "What are neural networks?",
    "Describe deep learning.",
    # Add more queries as needed
]

for _ in range(1000):
    query = random.choice(queries)
    app.rag(query, GenerationConfig(model="gpt-4o-mini"))

print("Preloading complete. You can now run analytics on this data.")
```

### 3.3 User-Level Analytics

To get analytics for a specific user:

```python
user_id = "your_user_id_here"

user_filter = FilterCriteria(filters={"user_analytics": "user_id"})
user_analysis = AnalysisTypes(analysis_types={
    "user_analytics": ["basic_statistics", "user_id"],
    "user_search_latencies": ["basic_statistics", "search_latency"]
})

user_analytics = app.analytics(user_filter, user_analysis)
print(f"Analytics for user {user_id}:")
print(user_analytics)
```

## 4. Log Analysis Tips

1. Look for ERROR or WARNING level logs first.
2. Check timestamps to correlate logs with observed issues.
3. Use tools like `grep`, `awk`, or `sed` to filter logs.
4. For large log files, use `less` with search functionality.

## 5. Log Aggregation Tools

Consider using log aggregation tools for more advanced setups:

1. ELK Stack (Elasticsearch, Logstash, Kibana)
2. Prometheus and Grafana
3. Datadog
4. Splunk

## Summary

R2R's logging, analytics, and observability features provide powerful tools for understanding and optimizing your RAG application. By leveraging these capabilities, you can:

- Monitor system performance in real-time
- Analyze trends in search and RAG operations
- Identify potential bottlenecks or areas for improvement
- Track user behavior and usage patterns
- Make data-driven decisions to enhance your application's performance and user experience

Remember to rotate logs regularly and set up log retention policies to manage disk space, especially in production environments.

For more advanced usage and customization options, consider joining the [R2R Discord community](https://discord.gg/p6KqD2kjtB) or referring to the detailed [R2R documentation](https://r2r-docs.sciphi.ai/).

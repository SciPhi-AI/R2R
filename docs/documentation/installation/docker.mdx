# R2R Docker Installation

This guide will walk you through installing and running R2R using Docker, which is the quickest and easiest way to get started.

## Prerequisites

- Docker installed on your system. If you haven't installed Docker yet, please refer to the [official Docker installation guide](https://docs.docker.com/engine/install/).

## Install the R2R CLI & Python SDK

First, install the R2R CLI and Python SDK:

```bash
pip install r2r
```

<Info>We are actively developing a distinct CLI binary for R2R for easier installation. Please reach out if you have any specific needs or feature requests.</Info>

## Start R2R with Docker

<AccordionGroup>
  <Accordion title="Cloud LLM RAG" icon="cloud" defaultOpen={true}>
  To start R2R with OpenAI as the default LLM inference and embedding provider:
      ```bash
      # Set cloud LLM settings
      export OPENAI_API_KEY=sk-...

      r2r serve --docker --config-name=default
      ```
  [Refer here](/documentation/configuration/llm) for more information on how to configure various LLM providers.
  </Accordion>
  <Accordion title="Local LLMs" icon="house" defaultOpen={false}>
      To start R2R with your local computer as the default LLM inference provider:
      ```bash
      r2r serve --docker --config-name=local_llm
      ```
    Then, in a separate terminal you will need to run Ollama to provide completions:
    ```bash
      ollama pull llama3.1
      ollama pull mxbai-embed-large
      ollama serve
    ```
    The code above assumes that Ollama has already been installed. If you have not yet done so, then refer to the official Ollama webpage [for installation instructions](https://ollama.com/download). For more information on local installation, [refer here](/documentation/local-rag).
  </Accordion>
  <Accordion title="Custom Configuration" icon="gear" defaultOpen={false}>
    R2R offers flexibility in selecting and configuring LLMs, allowing you to optimize your RAG pipeline for various use cases. Execute the command below run deploy R2R with your own custom configuration:
      ```bash
      r2r serve --config-path=/abs/path/to/my_r2r.toml
      ```

    Learn in detail how to [configure your deployment here](/documentation/configuration).
  </Accordion>
</AccordionGroup>

<Note>
Hatchet (RabbitMQ based orchestration), Neo4j, and Postgres come bundled into the R2R Docker by default. If you prefer to use your own deployments of either Neo4j or Postgres, you may optionally include the flags `--exclude-neo4j` and/or `--exclude-postgres`.

Excluding postgres requires that a valid configuration be selected, such as `core/configs/neo4j_kg_no_vector_postgres.toml`.
</Note>

The above command will automatically pull the necessary Docker images and start all the required containers, including `R2R`, `Hatchet`, `Neo4j`, and `Postgres+pgvector`.

The end result is a live server at http://localhost:7272 serving the [R2R API](/api-reference/introduction).

In addition to launching a RESTful API, the R2R Docker also launches a applications at `localhost:7273` and `localhost:7274`, which you can [read more about here](/cookbooks/application).

### Stopping R2R

Safely stop your system by running `r2r docker-down` to avoid potential shutdown complications.

## Next Steps

After successfully installing R2R:

1. **Verify Installation**: Ensure all components are running correctly by accessing the R2R API at http://localhost:7272/v2/health.

2. **Quick Start**: Follow our [R2R Quickstart Guide](/documentation/quickstart) to set up your first RAG application.

3. **In-Depth Tutorial**: For a more comprehensive understanding, work through our [R2R Walkthrough](/cookbooks/walkthrough).

4. **Customize Your Setup**: Configure R2R components with the [Configuration Guide](/documentation/configuration).

If you encounter any issues during installation or setup, please use our [Discord community](https://discord.gg/p6KqD2kjtB) or [GitHub repository](https://github.com/SciPhi-AI/R2R) to seek assistance.

# LLM Provider Configuration Guide

R2R supports multiple Language Model (LLM) providers, allowing users to easily switch between different models and providers based on their requirements. The framework provides an abstract base class `LLMProvider` that defines a common interface for interacting with LLMs, ensuring consistency and flexibility.

## Supported LLM Providers

R2R currently supports the following LLM providers:

- OpenAI
- LiteLLM (default)
  - OpenAI
  - Anthropic
  - Vertex AI
  - HuggingFace
  - Azure OpenAI
  - Ollama
  - Together AI
  - Openrouter
- LLamaCpp

## LiteLLM Provider (Default)

The `LiteLLM` class is the default implementation of the `LLMProvider` that integrates with various LLM providers through the LiteLLM library. LiteLLM provides a unified interface to interact with different LLMs using a consistent API.

Key features of the `LiteLLM` implementation:

- Supports multiple LLM providers, including OpenAI, Anthropic, Vertex AI, HuggingFace, Azure OpenAI, Ollama, Together AI, and Openrouter.
- Allows switching between different LLMs by setting the appropriate environment variables and specifying the desired model.
- Provides automatic prompt template translation for certain providers (e.g., Together AI's Llama2 variants).
- Supports registering custom prompt templates for specific models.

To use the LiteLLM provider with a specific LLM provider, you need to set the appropriate environment variables. Here are some examples:

- OpenAI:
  - Set the `OPENAI_API_KEY` environment variable with your OpenAI API key.
- Anthropic:
  - Set the `ANTHROPIC_API_KEY` environment variable with your Anthropic API key.
- Together AI:
  - Set the `TOGETHERAI_API_KEY` environment variable with your Together AI API key.
- Ollama:
  - Ensure that your Ollama server is running and accessible.

Refer to the LiteLLM documentation for detailed instructions on setting up each provider.

## OpenAI Provider

The `OpenAILLM` class is an implementation of the `LLMProvider` that integrates with the OpenAI API for generating completions.

Key features of the `OpenAILLM` implementation:

- Initializes the OpenAI client using the provided API key.
- Supports both non-streaming and streaming completions.
- Allows passing additional arguments to the OpenAI API.

To use the OpenAI provider, you need to set the `OPENAI_API_KEY` environment variable with your OpenAI API key.


Here's the LlamaCpp section added to the LLM Provider Configuration Guide:

## LlamaCpp Provider

The `LlamaCPP` class is an implementation of the `LLMProvider` that integrates with the LlamaCpp library for generating completions using local models.

Key features of the `LlamaCPP` implementation:

- Initializes the LlamaCpp client using the provided model path and model name.
- Supports both non-streaming and streaming completions.
- Allows passing additional arguments to the LlamaCpp client.

To use the LlamaCpp provider, you need to install the `llama-cpp-python` package:

```
pip install llama-cpp-python
```

### Configuring LlamaCpp

To configure the LlamaCpp provider, you need to set the following options in the `language_model` section of the configuration file (`config.json`):

- `provider_name`: Set it to `"llamacpp"` to use the LlamaCpp provider.
- `model_path`: Specify the path where the LlamaCpp models are located. If not provided, it defaults to `~/.cache/models`.
- `model_name`: Specify the name of the LlamaCpp model to use. If not provided, it defaults to `"tinyllama-1.1b-chat-v1.0.Q2_K.gguf"`.

Example configuration:

```json
"language_model": {
    "provider": "llamacpp",
    "model_path": "/path/to/models",
    "model_name": "custom_model.gguf"
}
```


## Configuring Remote LLM Providers

To configure a specific LLM provider, you need to set the appropriate environment variables and update the `language_model` section of the configuration file (`config.json`).

### OpenAI

- Set the `OPENAI_API_KEY` environment variable with your OpenAI API key.

### LiteLLM

- Set the appropriate environment variables for the desired LLM provider, as mentioned in the previous section.

Make sure to update the `language_model` section of the configuration file (`config.json`) with the desired provider and any additional provider-specific settings.

## Using the RAG Completion Endpoint

Once you have configured your LLM provider and set the appropriate environment variables, you can use the `rag_completion` endpoint of the R2R application to generate completions.

The `rag_completion` endpoint accepts a `RAGQueryModel` as input, which includes the query, filters, limit, and generation configuration. The endpoint then calls the `run` method of the RAG pipeline to generate the completion.

If `stream` is set to `False`, the endpoint returns the `RAGPipelineOutput` containing the generated completion. If `stream` is set to `True`, the endpoint streams the completion chunks using `StreamingResponse`.


## Toggling Between LLM Providers

R2R uses a factory pattern to create instances of LLM providers based on the provided configuration. The `E2EPipelineFactory` class is responsible for creating the appropriate LLM provider instance using the `get_llm` method.

To toggle between different LLM providers, you need to update the `language_model` section of the configuration file (`config.json`). For example:

```json
"language_model": {
    "provider": "litellm",
}
```

By changing the `provider` value to `"openai"` or `"litellm"`, you can switch between the supported LLM providers.

## LLM Provider Abstraction

R2R provides an abstract base class `LLMProvider` that defines the interface for interacting with LLMs. This abstraction allows for a consistent and unified way to perform operations such as getting completions and streaming completions.

The `LLMProvider` class defines the following abstract methods:

- `get_completion(messages: list[dict], generation_config: GenerationConfig, **kwargs) -> ChatCompletion`: Gets a chat completion from the provider.
- `get_completion_stream(messages: list[dict], generation_config: GenerationConfig, **kwargs) -> ChatCompletionChunk`: Gets a completion stream from the provider.


## Conclusion

R2R provides a flexible and extensible framework for integrating with different LLM providers. By abstracting the LLM interface through the `LLMProvider` base class, users can easily switch between providers by updating the configuration and setting the appropriate environment variables.

The framework supports both OpenAI and LiteLLM providers out of the box, with LiteLLM serving as the default provider. LiteLLM offers a unified interface to multiple LLM providers, allowing users to experiment with different models and choose the one that best suits their needs.

By leveraging the `rag_completion` endpoint, users can generate completions using the configured LLM provider, either in a non-streaming or streaming fashion.

With this modular approach, R2R enables users to seamlessly integrate and switch between LLM providers without modifying the core application code.
## Default Configuration

During the example pipeline creation, a default `config.json` is loaded and passed to the pipeline. It provides settings for the following services:

- R2R Application settings
- Embedding settings
- LLM settings
- Vector Database provider
- Knowledge Graph provider
- Evaluation provider
- Ingestion provider
- Prompt provider
- Logging

The default values for the config are shown below:

```json
{
  "app": {
    "max_logs_per_request": 100,
    "max_file_size_in_mb": 32
  },
  "completions": {
    "provider": "litellm"
  },
  "embedding": {
    "provider": "openai",
    "base_model": "text-embedding-3-small",
    "base_dimension": 512,
    "batch_size": 128,
    "text_splitter": {
      "type": "recursive_character",
      "chunk_size": 512,
      "chunk_overlap": 20
    },
    "rerank_model": "None"
  },  
  "eval": {
    "provider": "local",
    "llm": {
      "model": "gpt-4o",
      "provider": "openai"
    }
  },
  "ingestion":{
    "excluded_parsers": {
      "mp4": "default"
    }
  },
  "logging": {
    "provider": "local",
    "log_table": "logs",
    "log_info_table": "log_info"
  },
  "prompt": {
    "provider": "local"
  },
  "vector_database": {
    "provider": "local",
    "collection_name": "demo_vecs"
  }
}
```

The available options for each section are:

- **app**:
  - `max_logs_per_request`: Maximum number of logs to retrieve.
  - `max_file_size_in_mb`: Maximum file size allowed for ingestion (in MB).

- **completions**:
  - `provider`: `"litellm"`, `"openai"` (see [LLM Providers](/providers/llms.md) for more info).

- **embedding**:
  - `provider`: `"openai"`, `"sentence-transformers"`, or `"None"` (see [Embedding Providers](/providers/embeddings.md) for more info).
  - `base_model`: Embedding model to use (e.g., `"text-embedding-3-small"` for OpenAI).
  - `base_dimension`: Embedding dimension.
  - `batch_size`: Number of texts to embed in each batch.
  - `text_splitter`: Configuration for text splitting.
    - `type`: `"recursive_character"`.
    - `chunk_size`: Target chunk size in characters.
    - `chunk_overlap`: Number of overlapping characters between chunks.
  - `rerank_model`: Model used for re-ranking (optional).

- **eval**:
  - `provider`: `"local"` or `"None"`
  - `llm`: LLM configuration for evaluation.
    - `model`: Model to use (e.g., `"gpt-4o"`).
    - `provider`: Provider to use (e.g., `"openai"`).

- **kg**
  - `provider`: `"neo4j"` or `"None"`
  - `batch_size`: The number of samples to be batched to the LLM per call.
  - `text_splitter`: Specifications for the text splitter which creates fragments for kg production, `type`, `chunk_size`, and `chunk_overlap`.

- **ingestion**:
  - `excluded_parsers`: Specifies the parsers to exclude for different file types. Valid options include `csv`, `docx`, `html`, `json`, `md`, `pdf`, `pptx`, `txt`, `xlsx`, `.gif`, `.jpg`, `.jpeg`, `.svg`, `.mp3`, `.mp4`.

- **logging**:
  - `provider`: `"local"`, `"postgres"`, `"redis"`.
  - `log_table`: Name of the table to store logs.
  - `log_info_table`: Name of the table to store log info.

- **prompt**:
  - `provider`: `"local"`.

- **vector_database**:
  - `provider`: `"local"`, `"pgvector"` (see [Vector Database Providers](/providers/vector-databases.md) for more info).
  - `collection_name`: Name of the collection to store vector embeddings.

For more detailed information on configuring specific providers, please refer to the relevant documentation:

- [LLM Providers](/providers/llms.md)
- [Vector Database Providers](/providers/vector-databases.md)
- [Embedding Providers](/providers/embeddings.md)

These documentation files provide in-depth guides on setting up and using each provider, including any required environment variables and additional configuration options.

To launch the default pipeline with your own config, you may run the following code:

```python copy
from r2r import R2RConfig, R2RAppBuilder

config_path = PATH_TO_YOUR_CONFIG
config = R2RConfig.from_json(config_path=config_path)
app = R2RAppBuilder(config).build().app
```